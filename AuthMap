#!/usr/bin/env python3
# AuthMap.py
# 
#  An authenticated, source-driven application behavior mapper used to derive authorization expectations and detect enforcement gaps.
#  
#
#  python3 AuthMap.py --cookie session=xxxxxxxxxxxxxxxxxx --scope-mode same-host https://example.com
#  diff -u <old_outdir>/endpoints.json <new_outdir>/endpoints.json
#
#  jq -r 'keys[]' endpoints.json | sort -u > burp_urls.txt
#  cat burp_urls.txt | while read url; do                                   <-----recipe to dump results into burp
#  curl -k -x http://127.0.0.1:8080 "$url" 
#  done
#
# jq -r 'keys[]' user/endpoints.json | sort -u > user.txt
# jq -r 'keys[]' admin/endpoints.json | sort -u > admin.txt                           <---------discover BFLA and IDOR
# comm -13 user.txt admin.txt > adminonly.txt
# comm -23 user.txt admin.txt > suspicious_acl.txt



import argparse
import asyncio
import json
import re
import time
from collections import defaultdict, deque
from dataclasses import dataclass, asdict
from html import unescape
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse, urldefrag

import httpx
from bs4 import BeautifulSoup

DEFAULT_UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36"
DEFAULT_ACCEPT = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
DEFAULT_ACCEPT_LANG = "en-US,en;q=0.9"

# --- Extraction regexes (source-driven; no guessing) ---
# URLs inside JS: fetch("..."), axios.get("..."), $.ajax({url:"..."}), XHR.open("GET","...")
JS_URL_PATTERNS = [
    re.compile(r"""fetch\(\s*(['"])(/[^'"]+)\1"""),
    re.compile(r"""axios\.(get|post|put|delete|patch)\(\s*(['"])(/[^'"]+)\2""", re.IGNORECASE),
    re.compile(r"""\.open\(\s*(['"])(GET|POST|PUT|DELETE|PATCH)\1\s*,\s*(['"])(/[^'"]+)\3""", re.IGNORECASE),
    re.compile(r"""url\s*:\s*(['"])(/[^'"]+)\1""", re.IGNORECASE),
]

# Generic route-like string literals in JS bundles (kept conservative):
# - must start with /
# - avoid obvious static assets
ROUTE_LITERAL = re.compile(r"""(['"])(/[^'"]{2,200})\1""")
STATIC_EXT = re.compile(r"""\.(png|jpg|jpeg|gif|svg|ico|woff2?|ttf|eot|css|map)$""", re.IGNORECASE)

@dataclass
class Endpoint:
    url: str
    discovered_from: List[str]
    status: Optional[int] = None
    content_type: Optional[str] = None

def base_domain(host: str) -> str:
    # Simple base-domain heuristic (good enough for same-site scoping).
    # If you want PSL accuracy, swap in `publicsuffix2`, but that adds a dep.
    parts = host.split(".")
    if len(parts) <= 2:
        return host
    return ".".join(parts[-2:])

def in_scope(candidate: str, root: str, scope_mode: str) -> bool:
    cu, ru = urlparse(candidate), urlparse(root)
    ch = (cu.hostname or "").lower()
    rh = (ru.hostname or "").lower()
    if not ch or not rh:
        return False
    if scope_mode == "same-host":
        return ch == rh
    # same-site: include subdomains of base domain of root
    return ch == rh or ch.endswith("." + base_domain(rh)) or ch == base_domain(rh)

def normalize_url(found: str, current: str) -> Optional[str]:
    if not found:
        return None
    found = found.strip()
    # Ignore JS pseudo-URLs
    if found.startswith("javascript:") or found.startswith("mailto:") or found.startswith("tel:"):
        return None
    # Resolve relative URLs
    absu = urljoin(current, found)
    absu, _frag = urldefrag(absu)
    pu = urlparse(absu)
    if pu.scheme not in ("http", "https"):
        return None
    return absu

def extract_from_html(html: str, page_url: str) -> Tuple[Set[str], Set[str]]:
    """
    Returns:
      (page_links, script_links) discovered from HTML source.
    """
    soup = BeautifulSoup(html, "html.parser")
    links: Set[str] = set()
    scripts: Set[str] = set()

    # A) href/action/src extraction
    for a in soup.find_all("a"):
        href = a.get("href")
        nu = normalize_url(href, page_url)
        if nu:
            links.add(nu)

    for form in soup.find_all("form"):
        action = form.get("action")
        nu = normalize_url(action, page_url)
        if nu:
            links.add(nu)

    for el in soup.find_all(["iframe", "frame"]):
        src = el.get("src")
        nu = normalize_url(src, page_url)
        if nu:
            links.add(nu)

    for s in soup.find_all("script"):
        src = s.get("src")
        if src:
            nu = normalize_url(src, page_url)
            if nu:
                scripts.add(nu)

    # B) inline script URL extraction
    for s in soup.find_all("script"):
        if s.get("src"):
            continue
        txt = s.get_text() or ""
        txt = unescape(txt)
        for pat in JS_URL_PATTERNS:
            for m in pat.finditer(txt):
                # last group contains the path
                path = m.groups()[-1]
                nu = normalize_url(path, page_url)
                if nu:
                    links.add(nu)

    return links, scripts

def extract_routes_from_js(js_text: str, base_url: str) -> Set[str]:
    out: Set[str] = set()
    txt = unescape(js_text)

    # Direct patterns first
    for pat in JS_URL_PATTERNS:
        for m in pat.finditer(txt):
            path = m.groups()[-1]
            nu = normalize_url(path, base_url)
            if nu:
                out.add(nu)

    # Conservative fallback: route-like literals
    for m in ROUTE_LITERAL.finditer(txt):
        path = m.group(2)
        # drop obvious assets
        if STATIC_EXT.search(path):
            continue
        # drop extremely noisy patterns
        if path.count("{") or path.count("}"):
            continue
        nu = normalize_url(path, base_url)
        if nu:
            out.add(nu)
    return out

async def fetch(client: httpx.AsyncClient, url: str) -> Tuple[Optional[int], Optional[str], Optional[str]]:
    try:
        r = await client.get(url, follow_redirects=True)
        ct = r.headers.get("content-type", "")
        body = r.text if "text" in ct or "html" in ct or "javascript" in ct else None
        return r.status_code, ct, body
    except Exception:
        return None, None, None

async def crawl(root: str, cookie_kv: str, scope_mode: str, max_pages: int, concurrency: int, download_js: bool, outdir: str,
               user_agent: str, accept: str, accept_lang: str) -> None:
    t0 = time.time()
    ru = urlparse(root)
    if not ru.scheme:
        root = "https://" + root

    headers = {
        "User-Agent": user_agent,
        "Accept": accept,
        "Accept-Language": accept_lang,
    }
    cookies = {}
    # cookie_kv = "SESSION=..."
    if "=" in cookie_kv:
        k, v = cookie_kv.split("=", 1)
        cookies[k] = v
    else:
        raise SystemExit("--cookie must be like NAME=value (e.g., SESSION=abc)")

    limits = httpx.Limits(max_connections=concurrency, max_keepalive_connections=concurrency)
    timeout = httpx.Timeout(20.0, connect=10.0)

    discovered: Dict[str, Endpoint] = {}
    edges: Dict[str, Set[str]] = defaultdict(set)
    visited: Set[str] = set()
    q = deque([root])

    async with httpx.AsyncClient(headers=headers, cookies=cookies, timeout=timeout, limits=limits) as client:
        sem = asyncio.Semaphore(concurrency)

        async def worker(url: str):
            async with sem:
                status, ct, body = await fetch(client, url)
                return url, status, ct, body

        while q and len(visited) < max_pages:
            batch: List[str] = []
            while q and len(batch) < concurrency and len(visited) + len(batch) < max_pages:
                u = q.popleft()
                if u in visited:
                    continue
                if not in_scope(u, root, scope_mode):
                    continue
                batch.append(u)

            if not batch:
                break

            results = await asyncio.gather(*(worker(u) for u in batch))

            for url, status, ct, body in results:
                visited.add(url)
                if url not in discovered:
                    discovered[url] = Endpoint(url=url, discovered_from=[])
                discovered[url].status = status
                discovered[url].content_type = ct

                if not body:
                    continue

                # Only parse HTML pages for links; JS handling below
                if "html" in (ct or ""):
                    links, scripts = extract_from_html(body, url)

                    for nu in links:
                        if not in_scope(nu, root, scope_mode):
                            continue
                        edges[url].add(nu)
                        if nu not in discovered:
                            discovered[nu] = Endpoint(url=nu, discovered_from=[url])
                        else:
                            discovered[nu].discovered_from.append(url)
                        if nu not in visited:
                            q.append(nu)

                    # Optionally fetch same-scope JS bundles and extract routes
                    if download_js:
                        for jsu in scripts:
                            if not in_scope(jsu, root, scope_mode):
                                continue
                            # fetch JS bundle quickly
                            j_status, j_ct, j_body = await fetch(client, jsu)
                            if j_body and ("javascript" in (j_ct or "") or jsu.endswith(".js")):
                                js_routes = extract_routes_from_js(j_body, url)
                                for nu in js_routes:
                                    if not in_scope(nu, root, scope_mode):
                                        continue
                                    edges[url].add(nu)
                                    if nu not in discovered:
                                        discovered[nu] = Endpoint(url=nu, discovered_from=[f"js:{jsu}"])
                                    else:
                                        discovered[nu].discovered_from.append(f"js:{jsu}")
                                    if nu not in visited:
                                        q.append(nu)

    # Write outputs
    import os
    os.makedirs(outdir, exist_ok=True)

    endpoints_path = os.path.join(outdir, "endpoints.json")
    tree_path = os.path.join(outdir, "tree.json")
    summary_path = os.path.join(outdir, "summary.json")

    with open(endpoints_path, "w") as f:
        json.dump({k: asdict(v) for k, v in discovered.items()}, f, indent=2)

    with open(tree_path, "w") as f:
        json.dump({k: sorted(list(v)) for k, v in edges.items()}, f, indent=2)

    summary = {
        "root": root,
        "scope_mode": scope_mode,
        "download_js": download_js,
        "visited_count": len(visited),
        "discovered_count": len(discovered),
        "edge_count": sum(len(v) for v in edges.values()),
        "max_pages": max_pages,
        "concurrency": concurrency,
        "elapsed_sec": round(time.time() - t0, 2),
        "outdir": outdir,
    }
    with open(summary_path, "w") as f:
        json.dump(summary, f, indent=2)

def main():
    ap = argparse.ArgumentParser(description="Fast source-driven authenticated endpoint discovery (no clicking, no guessing).")
    ap.add_argument("root", help="Root URL to crawl (e.g., https://x.example.com/)")
    ap.add_argument("--cookie", required=True, help="Cookie as NAME=value (e.g., SESSION=abc123). No login performed.")
    ap.add_argument("--scope-mode", choices=["same-host", "same-site"], default="same-site",
                    help="same-host stays on exact host. same-site includes subdomains under the root base domain.")
    ap.add_argument("--max-pages", type=int, default=1500)
    ap.add_argument("--concurrency", type=int, default=25)
    ap.add_argument("--download-js", action="store_true",
                    help="Fetch same-scope JS bundles and extract route/API paths from their source.")
    ap.add_argument("--outdir", default=f"authgrab_out/fast_{int(time.time())}")

    # Hard-coded defaults with override capability (as requested)
    ap.add_argument("--user-agent", default=DEFAULT_UA)
    ap.add_argument("--accept", default=DEFAULT_ACCEPT)
    ap.add_argument("--accept-language", default=DEFAULT_ACCEPT_LANG)

    args = ap.parse_args()
    asyncio.run(crawl(
        root=args.root,
        cookie_kv=args.cookie,
        scope_mode=args.scope_mode,
        max_pages=args.max_pages,
        concurrency=args.concurrency,
        download_js=args.download_js,
        outdir=args.outdir,
        user_agent=args.user_agent,
        accept=args.accept,
        accept_lang=args.accept_language
    ))

if __name__ == "__main__":
    main()
